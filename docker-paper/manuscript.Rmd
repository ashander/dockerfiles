---
title: "An introduction to Docker for reproducible research"
author: 
  - name: Carl Boettiger
    affiliation:
      l1: Center for Stock Assessment Research, 
      l2: 110 Shaffer Rd, Santa Cruz, CA 95050, USA 
    email: cboettig(at)gmail.com
    footnote: Corresponding author
abstract: |
  As computational work becomes more and more integral to many aspects of
  scientific research, computational reproducibility has become an issue
  of increasing importance to computer systems researchers and domain
  scientists alike. Though computational reproducibility seems more
  straight forward than replicating physical experiments, the complex
  and rapidly changing nature of computer environments makes being able
  to reproduce and extend such work a serious challenge.  In this paper,
  I explore common reasons that code developed for one research project
  cannot be successfully executed or extended by subsequent researchers.
  I review current approaches to these issues, including virtual machines
  and workflow systems, and their limitations. I then examine how the
  popular emerging technology Docker combines several areas from systems
  research - such as operating system virtualization, cross-platform
  portability, modular re-usable elements, versioning, and a 'DevOps'
  philosophy, to address these challenges.  I illustrate this with several
  examples of Docker use with a focus on the R statistical environment.

Keywords:
  - Docker
  - virtualization
  - reproducibility

bibliography: components/references.bib
csl: components/acm-sig-proceedings.csl

output: 
  pdf_document:
    template: components/sigproc-sp.tex
    keep_tex: true
    fig_caption: true
---


<!-- Excerpt from the CALL FOR PAPERS:
There is increasing interest in promoting repeatable and reproducible research
in computer science.  A community gains confidence in scientific claims when
the experiments supporting those claims are published, examined, and repeated.
Repeating research also helps a community expand on previous results and
discover new directions for investigation.  Repetition requires that the
artifacts used in an experiment be made available to, and made usable by,
investigators.  In other words, artifacts must be shared.  Possible artifacts
include not only binaries, source code, and datasets but also computing and
networking infrastructure such as testbeds and clouds.

Systems researchers face special challenges in the areas of repeatability and
artifact sharing.  Some arise from the nature of systems software, which is
typically complex and rife with internal and external dependencies.  Other
issues arise from execution environments: for example, to reproduce performance
and scalability results, one may need access to actual and specific hardware.
Still more systems-specific challenges arise from the difficulty of measuring
properties of interest, such as time and energy consumption.

Suggested paper topics include, but are not limited to:

  * novel ways to manage systems research artifacts, dependencies, workflows,
    and data collection
  * testbeds for repeatable research and/or publishing executable artifacts
  * "rich publications" that connect papers to software and data
  * repeatability in distributed and nondeterministic systems
  * mitigating obstacles to artifact sharing, repeatability, and reproduction
  * repeatability challenges specific to systems
  * measurement bias in systems experiments
  * experiences and outcomes from systems repetition and reproduction studies
  * improving scientific practice in systems research
  * incentives for producing and sharing high-quality research artifacts
-->

# Introduction 

Reproducible research has received an increasing level of attention
throughout the scientific community [_e.g._ @Peng_2011; @Nature_2012]
and the public at large [_e.g._ @The_Economist_2013]. All steps of the
scientific process, from data collection and processing, to analyses,
visualizations and conclusions depend ever more on computation and
algorithms, _computational reproducibility_ has received particular
attention [@Merali_2010].  Though in principle this algorithmic dependence
should make such research easier to reproduce -- computer codes being
both more portable and potentially more precise to exchange and run
than experimental methods -- in practice this has led to an ever larger
and more complex black box that stands between what was actually done
and what is described in the literature.  Crucial scientific processes
such as replicating the results, extending the approach or testing the
conclusions in other contexts, or even merely installing the software
used by the original researchers can become immensely time-consuming if
not impossible.



### Systems research & reproducibility

Systems research has long concerned itself with the issues of
computational reproducibility and the technologies that can facilitate
those objectives [_e.g._ @Eide_2010, @Harji_2013]. Docker is a new but already
very popular open source tool that combines many of these approaches
in a user friendly implementation, including: (1) performing Linux
container (LXC) based operating system (OS) level virtualization, (2)
portable deployment of containers across platforms, (3) component reuse,
(4) sharing, (5) archiving, and (6) versioning of container images. While
Docker's market success has largely focused on the needs of businesses
in deploying web applications and the potential for a lightweight
alternative to full virtualization, these features have potentially
important implications for systems research in the area of scientific
reproducibility. 

In this paper, I seek to address two audiences. First, that of the domain
scientist, those conducting research in ecology, bioinformatics, economics,
psychology and so many other disciplines in which computation plays an 
ever-increasing role. I seek to help this audience become more aware of the 
concerns and challenges in making these computations more reproducible, extensible, 
and portable to other researchers, and highlight Docker as platform to address 
these challenges. The second audience is that of the computer systems researchers:
readers familiar with both the challenges to reproducibility (versioning,
software dependencies, etc.) and the technical components underlying Docker (virtualization,
LXC containers, jails, hashes, etc.), but who may be unfamiliar with Docker software. 
I hope the domain scientist will be motivated to try Docker as a tool to address
reproducibility. Meanwhile, I expect the systems researcher to see Docker technology as
an area ripe for further systems research (e.g. to what extent can differences between 
Linux kernels frustrate reproducibility in a Docker image?) rather than as a tool
for their own study (because Docker applies only to software at the level above
the Linux kernel, it will be of little use in making studies of differences in 
kernel behavior, hardware performance, execution times, and so forth reproducible.)



### A cultural problem

It is worth observing from the outset that the primary barrier to
computational reproducibility in many domain sciences has nothing
to do with the technological approaches discussed here, but stems rather from a
reluctance to publish the code used in generating the results in the
first place [@Barnes_2010]. Despite extensive evidence to the contrary
[_e.g._ @Ince_2012], many researchers and journals continue to assume that
summary descriptions or pseudo-code provide a sufficient description of
computational methods used in data gathering, processing, simulation,
visualization, or analysis.  Until such code is available in the first
place, one cannot even begin to encounter the problems that the approaches
discussed here set out to solve. As a result, few domain researchers
may be fully aware of the challenges involved in effectively re-using
published code.

A lack of requirements or incentives no doubt plays a crucial role in
discouraging sharing [@Barnes_2010; @Stodden_2013]. Nevertheless,
it is easy to underestimate the significant barriers raised by a lack
of familiar, intuitive, and widely adopted tools for addressing the
challenges of computational reproducibility. Surveys and case studies find
that a lack of time, more than innate opposition to sharing, discourages
researchers from providing code [@Stodden_2010; @FitzJohn_2014].


## Four technical challenges 

By restricting ourselves to studies of where code has been made
available, I will sidestep for the moment the cultural challenges
to reproducibility so that I may focus on the technical ones; in
particular, those challenges for which improved tools and techniques
rather than merely norms of behavior can contribute substantially to
improved reproducibility.

Studies focusing on code that has been made available with scientific
publications regularly find the same common issues that pose substantial
barriers to reproducing the original results or building on that code
[_e.g._ @Lapp_2014; @Garijo_2013; @Gilbert_2012; @Collberg_2014;
@Krishnamurthi_2014], which I attempt to summarize as follows.

### 1. **"Dependency Hell"**

A recent study by researchers at the University of Arizona found that
less than 50% of software could even be successfully built or installed
[@Collberg_2014]. Though sufficiently knowledgeable users may be able
to overcome the issues in at least some of these cases [^6], similar
results are seen in an ongoing effort by other researchers to replicate
that study [@Krishnamurthi_2014], and has also been observed in other
indepedent studies [@Lapp_2014; @Garijo_2013].  Installing or building
software necessary to run the code in question assumes the ability to
recreate the computational environment of the original researchers.

[^6]: See discussion at https://gist.github.com/pbailis/9629050 and https://gist.github.com/samth/9641364

Differences in numerical evaluation, such as arise in floating point
arithmetic or even ambiguities in standardized programming
languages ("order-of-evaluation" problems) can be responsible for
differing results between or even within the same computational platform
[@Ince_2012]. Such issues make it difficult to restrict the true dependencies
of the code to higher level environments such as that of a given scripting
language, independent of the underlying OS or even hardware itself.

### 2.  **Imprecise documentation**

Documentation on how to install and run code associated with published
research is another frequent barrier to replication. A study by Lapp [@Lapp_2014] found
this impairs a researcher's ability to install and build the software
necessary, as even small holes in the documentation were found to be
major barriers, particularly for "novices" [Sensu @Garijo_2013] --
where novices may be experts in nearby languages but unfamiliar with
the package managers and other tools of the language involved. This
same problem is discussed in [@Clark_2014].  Imprecise documentation
goes well beyond issues of the software environment itself: incomplete
documentation of parameters involved meant as few as 30% of analyses
($n=34$) using the popular software STRUCTURE could be reproduced in the
study of [@Gilbert_2012].


### 3.  **Code rot**

Software dependencies are not static elements, but receive regular
updates that may fix bugs, add new features or deprecate old features
(or even entire dependencies themselves).  Any of these changes can
potentially change the results generated by the code.  As some of these
changes may indeed resolve valid bugs or earlier problems with underlying
code, it will often be insufficient to demonstrate that results can be
reproduced when using the original versions, a problem sometimes known
as "code rot."  Researchers will want to know if the results are robust
to the changes. The case studies in [@Lapp_2014] provide examples of
these problems.

### 4. **Barriers to adoption and reuse in existing solutions**

Technological solutions such as workflow software, virtual machines,
continuous integration services, and best practices from software
development would address many of the issues frequently frustrating
reproducibility. However, researchers face significant barriers to entry
in learning these tools and approaches which are not part of their typical
curriculum, or lack incentives commensurate with the effort required
[@Joppa_2013; @FitzJohn_2014].

<!-- Obviously reproducible research barriers go much deeper than this,
code that is not modular, not documented, not functionalized or otherwise
easy to read and follow are all obstacles to resuse.-->

<!-- A wide range of proposals, I focus on those at the operating system level -->
Though a wide variety of approaches exists to work around these challenges,
few operate on a low enough level to provide a general solution. Clark _et al._ [@Clark_2014]
provide an excellent description of this situation:

> In scientific computing the environment was commonly managed via
Makefiles & Unix-y hacks, or alternatively with monolithic software
like Matlab. More recently, centralized package management has provided
curated tools that work well together. But as more and more essential
functionality is built out across a variety of systems and languages,
the value -- and also the difficulty -- of coordinating multiple tools
continues to increase. Whether we are producing research results or web
services, it is becoming increasingly essential to set up new languages,
libraries, databases, and more.

There are two dominant approaches to this issue of coordinating multiple
tools: Workflows and Virtual Machines (VMs). 

# Current approaches 

<!-- FIXME Doesn't say much about the approaches. Not clear what a workflow is... -->

Two dominant paradigms have emerged to address these issues so far:
workflow software [_e.g._ @Altintas_2004; @Hull_2006]  and
virtual machines [_e.g._ @Dudley_2010; @Howe_2012]. 
Workflow software provides very elegant technical solutions to the
challenges of communication between diverse software tools, capturing
provenance in graphically driven interfaces, and handling issues from 
versioning dependencies to data access.  Workflow solutions are often 
built by well-funded collaborations between domain scientists and computer
scientists, and can be very successful in the communities within which 
they receive substantial adoption. Nonetheless, most workflow systems 
struggle with relatively low total adoption overall [@Gil_2007; @Dudley_2010].

Dudley & Butte [@Dudley_2010] give several reasons that such comprehensive workflow
systems have not been more successful:

> (i) efforts are not rewarded by the current academic research and
funding environment; (ii) commercial software vendors tend to protect
their markets through proprietary formats and interfaces; (iii)
investigators naturally tend to want to 'own' and control their
research tools; (iv) even the most generalized software will not be
able to meet the needs of every researcher in a field; and finally (v)
the need to derive and publish results as quickly as possible precludes
the often slower standards-based development path.

In short, workflow software expects a new approach to computational research. 
In contrast, virtual machines (VMs) offer a more direct approach. Since
the computer Operating System (OS) already provides the software layer
responsible for coordinating all the different elements running on the computer,
the VM approach captures the OS and everything running on it whole-cloth.
To make this practical, Dudley & Butte [@Dudley_2010] and Howe [@Howe_2012] both
propose using virtual machine images that will run on the cloud, such as Amazon's
EC2 system, which is already based upon this kind of virtualization.

Critics of the use of VMs to support reproducibility highlight that the 
approach is too much of a black box and thus ill suited for reproducibility [@Watson_2012].
While the approach sidesteps the need to either install or even document the
dependencies, this also makes it more difficult for other researchers to
understand, evaluate, or alter those dependencies.  Moreover, other research
cannot easily build on the virtual machine in a consistent and scalable way. 
If each study provided its own virtual machine, any pipeline combining the 
tools of multiple studies would quickly become impractical or impossible 
to implement. 

<!-- Scripted provisioning: a solution to the weaknesses of virtual machines -->
## A "DevOps" approach

The problems highlighted here are not unique to _academic_ software,
but impact software development in general. While the academic research
literature has frequently focused on the development of workflow software
dedicated to particular domains, or otherwise to the use of virtual
machines, the software development community has recently emphasized a
philosophy (rather than a particular tool), known as _Development_ and
Systems _Operation_, or more frequently just "DevOps." The approach is
characterized by scripting, rather than documenting, a description of the
necessary dependencies for software to run, usually from the Operating
System (OS) on up. Clark _et al._ [@Clark_2014] describe the DevOps approach along
with both its relevance to reproducible research and examples of its
use in the academic research context. They identify the difficulties
I have discussed so far in terms of effective documentation:


> Documentation for complex software environments is stuck between two
opposing demands. To make things easier on novice users, documentation
must explain details relevant to factors like different operating
systems. Alternatively, to save time writing and updating documentation,
developers like to abstract over such details.

The authors contrast this to the DevOps approach, where dependency 
documentation is _scripted_:

> A DevOps approach to "documenting" an application might consist of
providing brief descriptions of various install paths, along with scripts
or "recipes" that automate setup.

This elegantly addresses both the demand for simplicity of use (one
executes a script instead of manually managing the environmental setup)
and comprehensiveness of implementation. Clark _et al._ [@Clark_2014] are careful to note
that this is not so much a technological shift as a philosophical one:

> The primary shift that's required is not one of new tooling, as most
developers already have the basic tooling they need. Rather, the needed
shift is one of philosophy.

Nevertheless, a growing suite of tools designed explicitly for this
purpose have rapidly replaced the use of general purpose tools (such
as Makefiles, bash scripts) to become synonymous with the DevOps
philosophy.  Clark _et al._ [@Clark_2014] reviews many of these DevOps tools, their
different roles, and their application in reproducible research.


I focus the remainder of this paper on one of the most recent and
rapidly growing among these, called Docker, and the role it can play
in reproducible research. Docker offers several promising features
for reproducibility that go beyond the tools highlighted in [@Clark_2014].
Nevertheless, my goal in focusing on this technology is not to
promote a particular solution, but to anchor the discussion of technical
solutions to reproducibility challenges in concrete examples.

<!-- Link on how to install Docker -->
<!-- Examples here are not meant as a tutorial to Docker, but 
rather to illustrate concepts a reader should be able to follow whether 
or not they have any previous knowledge of Docker and its commands -->

# Docker


<!-- Docker is an open source (Apache Version 2) software platform written in
the Go programming language.-->

Docker is an open source project that builds on many
long-familiar technologies from operating systems research:
LXC containers, virtualization of the OS, and a hash-based
or git-like versioning and differencing system, among others (see
[docs.docker.com/faq](https://docs.docker.com/faq/#what-does-docker-add-to-just-plain-lxc)
for an excellent overview of what Docker adds to plain LXC). The
official documentation [docs.docker.com](https://docs.docker.com)
already provides a thorough introduction in how to use Docker
software; here my focus is on describing the implications this
has for reproducible research.  Readers may also find it helpful
to see more detailed examples of using Docker to capture, share,
and interact with a specific computational environments. Some such
examples, along with more detailed documentation on use can be found at
[github.com/rocker-org](https://github.com/rocker-org/rocker).


I introduce the most relevant concepts from Docker through the context
of the four challenges for reproducible research I have discussed above.
In brief, these challenges can be addressed by distributing a Dockerfile
capable of reconstructing the researcher's development environment,
ideally along with depositing the corresponding binary Docker image in
an appropriate repository.


### 1. Docker images: resolving 'Dependency Hell' 

A Docker based approach works similarly to a virtual machine image in
addressing the dependency problem by providing other researchers with
a binary image in which all the software has already been installed,
configured and tested. (A machine image can also include all data files
necessary for the research, which may simplify the distribution of data.)

A key difference between Docker images and other virtual machines is 
that the Docker images share the Linux kernel with the host machine.
For the end user the primary consequence of this is that any Docker
image must be based on a Linux system with Linux-compatible software, which
includes R, Python, Matlab, and most other scientific programming needs.[^4] 

Sharing the Linux kernel makes Docker more light-weight and higher
performing than complete virtual machines -- a typical desktop computer
could run no more than a few virtual machines at once but 
would have no trouble running 100's of Docker containers (a container
is simply the term for running instance of an image). This feature
has made Docker particularly attractive to industry and is largely
responsible for the immense popularity of Docker. For our purposes this
is a nice bonus, but the chief value to reproducible research lies
in other aspects.

[^4]: Note that re-distribution of an image in which proprietary software
has been installed will be subject to any relevant licensing agreement.

<!-- 
- image sharing (hub)
- image linking
--> 

### 2. Dockerfiles: Resolving imprecise documentation

Though Docker images can be created interactively, this leaves little
transparent record [^3] of what software has been installed and how.
Dockerfiles provide a simple script (similar to a Makefile) that defines
exactly how to build up the image, consistent with the DevOps approach
I mentioned previously.

With a syntax that is simpler than other provisioning tools (_e.g._ Chef,
Puppet, Ansible) or Continuous Integration (CI) platforms (_e.g._ Travis CI,
Shippable CI); users need little more than a basic familiarity with shell
scripts and a Linux distribution software environment (_e.g._ Debian-based
`apt-get`) to get started writing Dockerfiles.

This approach has many advantages:

- While machine images can be very large (many gigabytes), a Dockerfile
is just a small plain text file that can be easily stored and shared.

- Small plain text files are ideally suited for use with a version
management system such as `subversion` or `git`, which can track any
changes made to the `Dockerfile`

- the `Dockerfile` provides a human readable summary of the necessary
software dependencies, environmental variables and so forth needed to
execute the code. There is little possibility of the kind of holes or 
imprecision in such a script that so frequently cause difficulty in
manually implemented documentation of dependencies. This approach
also avoids the burden of having to tediously document dependencies 
at the end of a project, since they are instead documented as they 
are installed by writing the `Dockerfile`.

- Unlike a `Makefile` or other script, the `Dockerfile` includes all
software dependencies down to the level of the OS, and is built by the
Docker `build` tool, making it very unlikely that the resulting build
will differ when being built on different machines. This is not to say
that all builds of a Dockerfile are bitwise identical.  In particular,
builds executed later will install more recent versions of the same
software, if available, unless the package managers used are explicitly
configured otherwise. I address this issue in the next section. 

- It is possible to add checks and tests following the commands for
installing the software environment, which will verify that the setup
has been successful.  This can be important in addressing the issue of
code-rot which I discuss next.

- It is straightforward for other users to extend or customize 
the resulting image by editing the script directly. 

[^3]: The situation is in fact slightly better than the virtual machine
approach because these changes are versioned. Docker provides tools
to inspect differences (diffs) between the images, and I can also roll
back changes to earlier versions. 

### 3. Tackling code-rot with image versions

As I have discussed above, changes to the dependencies, whether they are
the result of security fixes, new features, or deprecation of old software,
can break otherwise functioning code. These challenges can be significantly
reduced because Docker defines the software environment to a particular operating system 
and suite of libraries, such as the Ubuntu or Debian distribution.  Such 
distributions use a staged release model with `stable`, `testing` and `unstable` 
phases subjected to extensive testing to catch such potential problems [@Ooms_2013],
while also providing regular security updates to software within each stage.
Nonetheless, this cannot completely avoid the challenge of code-rot,
particularly when it is necessary to install software that is not (yet)
available for a given distribution.

To address this concern, one must archive a binary copy of the image
used at the time the research was first performed.  Docker provides a
simple utility to save an image as a portable `tarball` file that can
be read in by any other Docker installation, providing a robust way to
run the exact versions of all software involved.  By testing both the
`tarball` archive and the image generated by the latest Dockerfile,
Docker provides a simple way to confirm whether or not code rot has
effected the function of a particular piece of code. Binary Docker images
can be efficiently shared through the Docker Hub as well, as I describe
later.


### 4. Barriers to adoption and re-use

A technical solution, no matter how elegant, will be of little practical 
use for reproducible research unless it is both easy to use and adapt to
the existing workflow patterns of practicing domain researchers.

Though most of the concerns I have discussed so far can be addressed
through well-designed workflow software or the use of a DevOps approach
to provisioning virtual machines by scripts, neither approach has seen
widespread adoption by domain researchers, who work primarily in a
local rather than cloud-based environment using development tools native
to their personal operating system. To gain more widespread adoption,
reproducible research technologies must make it easier, not harder,
for a researcher to perform the tasks they are already doing (before 
considering any additional added benefits).

These issues are reflected both during the original research or development
phase and in any subsequent reuse. Another researcher may be less likely
to build on existing work if it can only be done by using a particular
workflow system or monolithic software platform with which they are
unfamiliar.  Likewise, a user is more likely to make their own computational
environment available for reuse if it does not involve a significant
added effort in packaging and documenting [@Stodden_2010]. 


Though Docker is not immune to these challenges, it offers an interesting 
example of a way forward in addressing these fundamental concerns. Here 
I highlight these features in turn:

- Integrating into local development environments
- Modular reuse
- Portable environments 
- Public repository for sharing
- Versioning 


## Integrating into local development environments

Due in part to the difficulty of moving large VM images around a network,
proponents of virtual machines for reproducible research often propose
that these machines would be available exclusively as cloud computing
environments [_e.g._ @Dudley_2010] rather than downloaded to a user's
personal computer. Though cloud computing offers some advantages such as
scalable resources for computationally intensive tasks, many researchers
prefer to work with tools installed locally on their personal computer,
at least during testing and development.  This can reduce costs, issues of
network latency, the ability to work off-line, and will be most familiar
to students and new developers.

It is possible to run virtual machines locally on most common laptop and
desktop computers, as demonstrated in the approach now being pioneered
at UC Berkeley [@Clark_2014].  This approach provides users with a pixel-identical
environment whether working locally or on the cloud, which has particular
advantages for student instruction [@Clark_2014]. However, it remains to be
seen if existing researchers will be willing to forgo native applications
for such tasks as file browsing, text editing, or version management and 
rely exclusively on this standardized virtual environment. 

In contrast, Docker's approach is optimized for a more integrated
workflow.  Rather than replace a user's existing toolchain with a
standardized virtual environment, editors and all, Docker is optimized at
the level of single applications. A developer can thus rely on familiar
tools while still ensuring that the execution of their code always occurs
on the standardized container environment, thus ensuring its portability
and reproducibility. This approach can be accomplished either by linking
volumes or directories between the container and the host, or simply
by instructing the Dockerfile to copy the code to the container (As we will
see, modular reuse makes this latter strategy efficient).

Rather than put the burden on the researcher to adopt
a very different workflow, the researcher can thus use their familiar editors,
etc., while immediately benefiting from the fact their computations can
be easily deployed on the cloud or the machines of collaborators without
any further effort than installing Docker software. The docker approach
is particularly well suited for moving between local and cloud platforms
when a web-based integrated development environment is available, such as
RStudio Server, or (to lesser extent) an iPython notebook.

On systems not already based on the Linux kernel (such
as Mac or Windows platforms), Docker is installed (see see
[docs.docker.com/installation](https://docs.docker.com/installation))
through means of a very small (about
24 MB) VM platform called `boot2docker`
([github.com/docker/boot2docker](https://github.com/docker/boot2docker)),.
While this poses an additional challenge for tight integration with desktop
tools, recent advances in Docker are rapidly bridging this gap as well. 
For example, Docker 1.3, released between drafts of this manuscript, supports
shared volumes on Macs ([blog.docker.com](http://blog.docker.com/2014/10/docker-1-3-signed-images-process-injection-security-options-mac-shared-directories/)). 


## Portable computation & sharing

A particular advantage of this approach is that the resulting
computational environment is immediately _portable_. LXC containers
by themselves are unlikely to run in the same way, if at all, across
different machines, due to differences in networking, storage,
logging and so forth. Docker handles the packaging and execution of
a container so that it works identically across different machines,
while exposing the necessary interfaces for networking ports, volumes,
and so forth.  This is useful not only for the purposes of reproducible
research, where other users may seek to reconstruct the computational
environment necessary to run the code, but is also of immediate value
to the researcher themselves. For instance, a researcher might want to
execute their code on a cloud server which has more memory or processing
power then their local machine, or would want a co-author to help debug a
particular problem.  In either case, the researcher can export a snapshot
of their running container:

```bash
docker export container-name > container.tar
```

and then run this identical environment on the cloud or collaborators' machine.

Sharing these images is further facilitated by the Docker Hub technology
([hub.docker.com](http://hub.docker.com)).  While Docker images tend
to be much smaller than equivalent virtual machines, moving around
even 100's of gigabytes can be a challenge.  The Docker Hub provides a
convenient distribution service, freely storing the pre-built images,
along with their metadata, for download and reuse by others. The Docker
Hub is a free service and an open source software product so that users
can run their own private versions of the Hub on their own servers, for
instance, if security of the data or the longevity of the public platform
is a concern.  Docker also supports *Automated Builds* through the Docker
Hub.  This acts as a kind of Continuous Integration (CI) service that
verifies the image builds correctly whenever the Dockerfile is updated,
particularly if the Dockerfile includes checks for the environment.

One can  share a public copy of the image just created by using the
`Docker push` command, followed by the name of the image using the command:

```bash
docker push username/r-recommended
```

If a Dockerfile is made available on a public code repository such as
[Github](https://github.com) or [Bitbucket](https://bitbucket.org),
the Hub can automatically build the image whenever a change is made to
the Dockerfile, making the `push` command unnecessary. A user can update
their local image using the `Docker pull <imagename>`, which downloads
any changes that have since been made to the copy of the image on the Hub.




## Re-usable modules

The approach of Docker offers a technical
solution to what is frequently seen as the primary weakness of the
standard VM approach to reproducibility - reusing and
remixing elements.  To some extent this is already addressed by the
DevOps approach of Dockerfiles, providing a scripted description
of the environment that can be tweaked and altered, but also includes
something much more fundamental to Docker.

The challenge to reusing VMs can be summarized as "you
can't install an image for every pipeline you want..." [@Watson_2012]. 
While providing a VM may make it easy for other researchers to run
a particular piece of software, it becomes very difficult to combine
multiple software components in future research if each must run inside
it's own VM. The lack of reusable, scalable modules in the VM model
poses a major barrier to future reuse.  In contrast, (as the analogy to 
shipping containers in the name might imply) Docker containers
are optimized for this kind of stacking and modular reuse.
There are at least three ways in which Docker supports this kind of
extensibility.

Most primitively, because Dockerfile itself provides a script for
creating the computational environment, future researchers can
extend or modify the resulting machine image by simply editing the
Dockerfile. This same approach is also possible for VM approaches that rely on 
DevOps tools [@Clark_2014]. In contrast to VMs, however, Docker is also
modular by design: both in how Dockerfiles are defined and how Docker
containers can be linked. 


First, Docker facilitates modular reuse by building one container on top
of another through the use of `FROM` directive in Dockerfiles. This acts
like a software dependency; but unlike other software, a Dockerfile must
have exactly one dependency (one `FROM` line). Particular version of
the dependency can be specified using the `:` notation, or omitted to
default to the latest version. To install software, Dockerfiles leverage
existing package managers on common Linux distributions such as `apt`
on Debian. These also permit installing either specific or only the
latest version.

<!--
Consider the following Dockerfile to illustrate this modular reuse. This
is the complete Dockerfile required to create the computational environment
required for a particular manuscript:

```bash
FROM rocker/ropensci:latest
MAINTAINER Carl Boettiger cboettig@ropensci.org
RUN git clone https://github.com/ropensci/RNeXML.git /rnexml
RUN Rscript -e 'devtools::install("/rnexml")'
```

The first line contains the FROM directive, indicating that this Dockerfile
builds on the existing Docker image `rocker/ropensci`, hosted on the Docker
Hub.  For this approach to be effective for reproducible one must take care 
that the base image and it's Dockerfile recipe are available from a persistant
archive.

The second line is optional, declaring minimal metadata regarding the 
contact information for the image maintainer. The third line copys the
project onto the image and the fourth installs the necessary software that
has not been provided by the base image. Projects using less standard
environments may require more instructions.  
-->

While a Dockerfile can have only a single `FROM` line, sometimes it may
be necessary to build on the computational environment provided by more
than one container. To address this, Docker defines a syntax for linking
multiple containers together.  This allows each container to act as a
building block providing just what is necessary to run one particular
service or element, and exposing just what is needed to link it together
with other blocks.  For instance, one could have one container running
a PostgreSQL database which serves data to another container running a
python environment to analyze the data:

<!--
```bash
docker run --name texlive -v /usr/local/texlive rocker/texlive
docker run -dP --volumes-from texlive \
-e PATH=$PATH:/usr/local/texlive/2014/bin/x86_64-linux/ \
rocker/rstudio 
```
-->


```bash
docker run -d --name db training/postgres
docker run -d -P --link db:db training/webapp python app.py
```
This separates the task for running the database (first line) from
the application used to analyze the data (second line), allowing a more
modular approach to reuse: another researcher could use the same database
container while connecting it to different scripts. 

Unlike the much more heavyweight virtual machine approach, a single
computer can easily run 100s of such services each in their own
container. A rapidly expanding ecosystem of software around Docker,
such as fig ([github.com/docker/fig](https://github.com/docker/fig))
facilitates the complexity of running multiple containers.  This feature
making it easy to break computational elements down into logically
reusable chunks that come, batteries included, with everything they need
to run reproducibly.


<!-- Transition -->

## Versioning 


In addition to version managing the Dockerfile, the images themselves are
versioned using a git-like hash system (_e.g._ see `Docker commit`, `docker
push`/`Docker pull`, `docker history`, `docker diff`).  Docker images and
containers have dedicated metadata specifying the date, author, parent
image, and other details (see `Docker inspect`). One can roll back an
image through the layers of history of its construction, then build off
an earlier layer, or roll back changes made interactively in a
container. For instance, here I inspect recent changes made to the
`ubuntu:14.04` image:

```bash
docker history ubuntu:14.04
```

One can identify an earlier version, and roll back to that version 
just by adjusting the Docker tag to match the hash of that version. 
For instance:

```bash
docker tag 25f ubuntu:14.04
```
If one now inspects the history, which shows that it now begins from this earlier point:

```bash
docker history ubuntu:14.04
```

This same feature also means that Docker can perform incremental
uploads and downloads that send only the differences between images,
(just like `git push` or `git pull` for git repositories), rather than
transfer the full image each time. 

<!-- Step back, big picture -->

# Conclusions


## Best Practices

The effectiveness of this approach for supporting reproducible research
nonetheless depends on how each of these features are adopted and 
implemented by individual researchers. I summarize a few of these practices here:

- _Use Docker containers during development_. A key feature of the Docker
approach is the ability to mimic as closely as possible the current
workflow and development practices of the user.  Code executing inside
a container on a local machine can appear identical to code running
natively, but with the added benefit that one can simply recreate or
snapshot and share the entire computational environment with a few simple
commands.  This works best if researchers set up their computational
environment in a container from the outset of the project.

- _Write Dockerfiles instead of installing interactive sessions_. As we
have noted already, Docker can be used in a purely interactive manner to
record and distribute changes to a computational environment.  However,
the approach is most useful for reproducible research when researchers
begin by defining their environment explicitly in the DevOps fashion by
writing a Dockerfile.

- _Adding tests or checks to the Dockerfile_. Dockerfile commands need
not be limited to installing software, but can also include execution.
This can help verify that an image has build successfully with all the
software necessary to run the research code of interest.

- _Use and provide appropriate base images_. Though Docker supports
modular design, it remains up to the researchers to take advantage of
it. An appropriate workflow might involve one Dockerfile that includes
all the software dependencies a researcher usually uses in the course of
their development, which can then be extended by separate Docker images
for particular projects.  Re-using existing images reduces the effort
required to set up an environment, contributes to the standardization
of computational environments within a field, and best leverages the
ability of Docker's distribution system to download only differences.

- _Share Docker images and Dockerfiles_. The Docker Hub significantly
reduces the barriers for making even large images (which can exceed the
file size limits of journals common scientific data repositories such
as Dryad and Figshare) readily available to other researchers.

- _Archive tarball snapshots_. Despite similar semantics to git, Docker's
versioning system works rather differently than version management
of code.  Docker can roll back layers[^5] that have been added to
an image, but not revert to the earlier state of a particular layer.
In consequence, to preserve a bitwise identical snapshot of a container
used to generate a given set of results, it is necessary to archive the
image tarball itself -- one can not simply rely on the Docker history
to recover an earlier state.

[^5]: Technically AUFS (advanced multi layered unification filesystem )
layers, see [wikipedia.org/wiki/aufs](http://en.wikipedia.org/wiki/aufs)

## Limitations and future developments

Docker has the potential to address shortcomings of certain existing
approaches to reproducible research challenges that stem from recreating
complex computational environments.  Docker also provides a promising
case study in other issues. Its versioning, modular design, portable
containers, and simple interface have proven successful in industry and
could have promising implications for reproducible research in scientific
communities. Nonetheless, these advances raise questions and challenges 
of their own. 

- Docker does not provide complete virtualization but relies on the 
Linux kernel provided by the host.  Systems research can provide insight
on what limitations to reproducibility this introduces [_e.g._  @Harji_2013].

- Docker is limited to 64 bit host machines, making it impossible to run on
older hardware (at this time).

- On Mac and Windows machines Docker must still be run in a fully
virtualized environment. Though the boot2docker tool streamlines this
process, it remains to be seen if the performance and integration with
the host machine's OS is sufficiently seamless or creates a barrier to
adoption by users on of these systems.

- Potential computer security issues may still need to be evaluated. Among other
changes, future support for digitally signing Docker images may make it easier
to build off of only trusted binaries. 

- Most importantly, it remains to be seen if Docker will be significantly
adopted by any scientific research or teaching community. 


## Further considerations 

<!-- Is this really necessary? Can it be more consise? -->

### Combining virtualization with other reproducible-research tools

<!-- Package managers -->

Using Docker containers to distribute reproducible research should be
seen as an approach that is synergistic with, rather than an alternative
to, other technical tools for ensuring computational reproducibility.
Existing tools for managing dependencies for a particular language [_e.g._ R
packages, Ruby gems, etc, see @Ooms_2014] can easily be employed within a
Docker-based approach, allowing the operating-systems level virtualization
to sidestep potential issues such as external library dependencies or
conflicts with existing user libraries.  Other approaches that facilitate
reproducible research also introduce additional software dependencies
and possible points of failure [@FitzJohn_2014]. One example includes
dynamic documents [_e.g._ @Leisch_2002; @Peng_2011; @Xie_2013] which embed
the code required to re-generate the results within the manuscript.  As a
result, it is necessary to package the appropriate typesetting libraries
(_e.g._ \LaTeX) along with the code libraries such that the document executes
successfully for different researchers and platforms.


### Impacting cultural norms? 

I noted at the outset that cultural expectations responsible for a
lack of code sharing practices in many fields are a far more extensive
primary barrier to reproducibility than the technical barriers
discussed here. Nevertheless, it may be worth considering how solutions
to these technical barriers can influence the cultural landscape as well.
Many researchers may be reluctant to publish code today because they
fear a it will be primarily a one-way street: more technical savvy
researchers than themselves can benefit from their hard work, while
they may not benefit from the work produced by others.  Lowering the
technical barriers to reuse provides immediate practical benefits
that make this exchange into a more balanced, two-way street. Another
concern is that the difficulty imposed in preparing code to be shared,
such as providing even semi-adequate documentation or support for other
users to be able to install and run it in the first place is too high
[@Stodden_2010]. Thus, lowering these barriers to re-use through
the appropriate infrastructure may also reduce certain cultural
barriers to sharing.

# Acknowledgements

CB acknowledges support from NSF grant DBI-1306697, and also from the
Sloan Foundation support through the rOpenSci project. CB also wishes
to thank Scott Chamberlain, Dirk Eddelbuettel, Rich FitzJohn, Yihui Xie,
Titus Brown, John Stanton-Geddes and many others for helpful discussions
about reproducibility, virtualization, and Docker that have helped shape
this manuscript.

# References 


<!-- These links don't appear in either pdf or rendered version, but provide some automatic linking on the Github rendering 
It would be nice if pandoc's reference syntax were closer to markdown's standard link syntax (e.g. always use [], bracket each reference)
-->


[@Lapp_2014]: https://storify.com/hlapp/reproducibility-repeatability-bigthink
[@Gilbert_2012]: 10.1111/j.1365-294X.2012.05754.x
[@Collberg_2014]: http://reproducibility.cs.arizona.edu/v1/tr.pdf
[@Brown_2014]: http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/
[@Vandewalle_2009]: http://doi.org/10.1109/MSP.2009.932122
[@Merali_2010]: http://doi.org/10.1038/467775a "Computational Science ... error: why scientific programming does not compute"
[@Gil_2007]: http://doi.org/10.1109/MC.2007.421 "Examining the Challenges of Scientific Workflows"
[@Hull_2006]: http://doi.org/10.1093/nar/gkl320 "Taverna: a tool for building and running workflows of services"
[@Altintas_2004]: http://doi.org/10.1109/SSDM.2004.1311241 "Kepler: an extensible system for design and execution of scientific workflows"
[@Howe_2012]: http://doi.org/10.1109/MCSE.2012.62
[@Dudley_2010]: http://doi.org/10.1038/nbt1110-1181 
[@Clark_2014]: https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl
[@Ooms_2013]: http://arxiv.org/abs/1303.2140v2 "Possible Directions for Improving Dependency Versioning in R"
[@Ooms_2014]: http://arxiv.org/abs/1406.4806 "The OpenCPU System: Towards a Universal Interface for Scientific Computing through Separation of Concerns"
[@Peng_2011]: http://doi.org/10.1126/science.1213847
[@Barnes_2010]: http://doi.org/10.1038/467753a
[@Ince_2012]: http://doi.org/10.1038/nature10836
[@Stodden_2014]: http://doi.org/10.3233/SJI-140818
[@Stodden_2010]: http://doi.org/10.2139/ssrn.1550193
[@Stodden_2013]: http://www.davidhbailey.com/dhbpapers/icerm-report.pdf "ICERM Working group report: Setting the Default to Reproducible: Reproducibility in Computational and Experimental Mathematics"
[@Nature_2012]: http://doi.org/10.1038/483509a
[@The_Economist_2013]: http://www.economist.com/news/leaders/21588069-scientific-research-has-changed-world-now-it-needs-change-itself-how-science-goes-wrong
[@van_der_Meulen_2008]: http://doi.org/10.1109/TSE.2008.70 "The effectiveness of software diversity in a large population of programs."
[@Watson_2012]: https://twitter.com/BioMickWatson/status/265037994526928896




```{r include=FALSE, eval=FALSE}
library("knitcitations")
options(citation_format = "pandoc")
citep(c(
"10.1371/journal.pone.0080278",
"10.1126/science.1213847",
"10.1038/467753a",
"10.1038/nature10836",
"10.1038/483509a",
"10.1038/467775a",
"10.1111/j.1365-294X.2012.05754.x",
"10.1109/MSP.2009.932122",
"10.1109/MC.2007.421",
"10.1093/nar/gkl320",
"10.1109/SSDM.2004.1311241",
"10.1109/MCSE.2012.62",
"10.1038/nbt1110-1181"))
citep("http://doi.org/10.3233/SJI-140818")
citep(c(
"10.2139/ssrn.1550193",
"10.1109/TSE.2008.70",
"http://arxiv.org/abs/1303.2140v2",
"http://arxiv.org/abs/1406.4806",
"https://storify.com/hlapp/reproducibility-repeatability-bigthink",
"http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/",
"https://berkeley.app.box.com/s/w424gdjot3tgksidyyfl",
"http://reproducibility.cs.arizona.edu/v1/tr.pdf",
"http://www.davidhbailey.com/dhbpapers/icerm-report.pdf",
"http://www.economist.com/news/leaders/21588069-scientific-research-has-changed-world-now-it-needs-change-itself-how-science-goes-wrong",
"https://twitter.com/BioMickWatson/status/265037994526928896"))


write.bibtex(file="components/references.bib")


```

<!-- Covered elsewhere in a more integrated fashion -->
<!--

Researchers familiar with LXC may be tempted to view the Docker
system as little more than a creative application and marketing of an
existing technology. However, the Docker approach adds several features
crucial for its use in reproducible research (adapted from [Docker
FAQ](https://docs.Docker.com/faq/#what-does-docker-add-to-just-plain-lxc)):

- *Portable deployment*: Docker defines a format for bundling and
exchanging containers such that they run identically across platforms.

- *Automatic builds*: Docker defines a format (Dockerfiles) that provide
a simple, scripted way to construct containers, following the DevOps
philosophy.


- *Versioning*: Docker provides git-like capibilities for tracking
changes to a container, inspecting the history, metadata, or diff between
versions, committing versions, rolling back changes, and incremential
uploads and downloads that send only the differences (like `git push`
or `git pull`).

- *Component reuse*: Any container can be used as the base for any other.

- *Sharing*: Docker provides a public registry (Docker Hub) where any
user can make images avaialble to any other. (The registry itself is
also open source so that it can be deployed privately).

Each of these elements have clear implications for reproducible research.

-->


